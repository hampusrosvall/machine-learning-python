{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# managing imports \n",
    "\n",
    "import keras \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "from conll_modules import CoNLLDictorizer, Token\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import operator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GLoVe embedding vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# loading pre trained word embeddings GloVe\n",
    "\n",
    "glove_dir = './glove.6B/'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_words = sorted(list(embeddings_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CoNLL corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-DOCSTART- -X- -X- O\\n\\nEU NNP B-NP B-ORG\\nrejects VBZ B-VP O\\nGerman JJ B-NP B-MISC\\ncall NN I-NP O\\nto T'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "\n",
    "def load_conll2003_en():\n",
    "    BASE_DIR = './data/'\n",
    "    train_file = BASE_DIR + 'train.txt'\n",
    "    dev_file = BASE_DIR + 'valid.txt'\n",
    "    test_file = BASE_DIR + 'test.txt'\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "train_sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sequences from the dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='ner', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words, Y_ner = build_sequences(train_dict)\n",
    "X_dev, Y_dev = build_sequences(dev_dict)\n",
    "X_test, Y_test = build_sequences(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(X_words[1])\n",
    "print(Y_ner[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counter number of unique words in vocabulary (embedding and corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_words = list(embeddings_index.keys())\n",
    "word_set = list(set([item for sublist in X_words for item in sublist]))\n",
    "ner_set = list(set([item for sublist in Y_ner for item in sublist]))\n",
    "total_words = set(embedding_words + word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build index where index 0 is kept for the unknown words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`idx_word = dict(enumerate(vocabulary_words, start = 2))` where 0 is for padding and 1 is for unknown word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_word_idx = dict(enumerate(total_words, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner_set, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity for words _table_, _france_ and _sweden_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('side', 0.6433666348457336),\n",
       " ('room', 0.654369056224823),\n",
       " ('bottom', 0.6559719443321228),\n",
       " ('place', 0.658237874507904),\n",
       " ('tables', 0.8021162748336792)]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_closest_words(word, n): \n",
    "    word_embedding = embeddings_index[word]\n",
    "    cosine_dict = {w: 1 - cosine(word_embedding, embeddings_index[w]) for w in embeddings_index}\n",
    "    return sorted(cosine_dict.items(), key=operator.itemgetter(1))[-(n + 1):-1]\n",
    "\n",
    "get_n_closest_words('table', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('austria', 0.7466837763786316),\n",
       " ('netherlands', 0.7468465566635132),\n",
       " ('finland', 0.7906494140625),\n",
       " ('norway', 0.8073249459266663),\n",
       " ('denmark', 0.8624401688575745)]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_closest_words('sweden', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paris', 0.7481586933135986),\n",
       " ('spain', 0.7557463049888611),\n",
       " ('britain', 0.7950528860092163),\n",
       " ('french', 0.8004377484321594),\n",
       " ('belgium', 0.8076422810554504)]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_closest_words('france', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402597, 100)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.random.random((len(total_words) + 2, 100))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in total_words: \n",
    "    if word in embeddings_index.keys(): \n",
    "        embedding_matrix[word_idx[word], :] = embeddings_index[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the $\\mathbf{X}$ and $\\mathbf{Y}$ Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[117788, 238614, 384043, 103649, 298006, 322581, 141446, 151281, 150277]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_words_idx = [list(map(lambda words: word_idx[words], words)) for words in X_words]\n",
    "Y_ner_idx = [list(map(lambda ner: ner_idx[ner], ner)) for ner in Y_ner]\n",
    "\n",
    "X_dev = [list(map(lambda words: word_idx.get(words, 1), words)) for words in X_dev]\n",
    "Y_dev = [list(map(lambda ner: ner_idx.get(ner, 1), ner)) for ner in Y_dev]\n",
    "\n",
    "X_test_unpadded = [list(map(lambda words: word_idx.get(words, 1), words)) for words in X_test]\n",
    "Y_test = [list(map(lambda ner: ner_idx.get(ner, 1), ner)) for ner in Y_test]\n",
    "\n",
    "X_words_idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_words_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building neural network architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_words_idx, maxlen)\n",
    "Y_train = pad_sequences(Y_ner_idx, maxlen)\n",
    "\n",
    "X_dev = pad_sequences(X_dev, maxlen)\n",
    "Y_dev = pad_sequences(Y_dev, maxlen)\n",
    "\n",
    "X_test = pad_sequences(X_test_unpadded, maxlen)\n",
    "Y_test = pad_sequences(Y_test, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0 117788 238614 384043 103649 298006 322581 141446 151281 150277]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 3 6 3 3 3 6\n",
      " 3 3]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])\n",
    "print(Y_train[1])\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical \n",
    "Y_train = to_categorical(Y_train, num_classes = len(ner_set) + 2)\n",
    "Y_dev = to_categorical(Y_dev, num_classes = len(ner_set) + 2)\n",
    "Y_test = to_categorical(Y_test, num_classes = len(ner_set) + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14987, 150)\n",
      "(14987, 150, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 150, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 150, 100)          20100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150, 11)           1111      \n",
      "=================================================================\n",
      "Total params: 40,280,911\n",
      "Trainable params: 21,211\n",
      "Non-trainable params: 40,259,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers \n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "NB_CLASSES = len(ner_set)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(len(total_words) + 2,\n",
    "EMBEDDING_DIM, mask_zero=True, input_length=maxlen))\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "# The default is True\n",
    "model.layers[0].trainable = False\n",
    "model.add(SimpleRNN(100, return_sequences=True))\n",
    "model.add(Dense(NB_CLASSES + 2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/10\n",
      "14987/14987 [==============================] - 14s 922us/step - loss: 0.0477 - acc: 0.8608 - val_loss: 0.0310 - val_acc: 0.9120\n",
      "Epoch 2/10\n",
      "14987/14987 [==============================] - 13s 888us/step - loss: 0.0235 - acc: 0.9266 - val_loss: 0.0238 - val_acc: 0.9345\n",
      "Epoch 3/10\n",
      "14987/14987 [==============================] - 13s 876us/step - loss: 0.0199 - acc: 0.9378 - val_loss: 0.0210 - val_acc: 0.9422\n",
      "Epoch 4/10\n",
      "14987/14987 [==============================] - 13s 879us/step - loss: 0.0180 - acc: 0.9440 - val_loss: 0.0213 - val_acc: 0.9419\n",
      "Epoch 5/10\n",
      "14987/14987 [==============================] - 13s 883us/step - loss: 0.0165 - acc: 0.9484 - val_loss: 0.0192 - val_acc: 0.9470\n",
      "Epoch 6/10\n",
      "14987/14987 [==============================] - 13s 885us/step - loss: 0.0154 - acc: 0.9515 - val_loss: 0.0193 - val_acc: 0.9450\n",
      "Epoch 7/10\n",
      "14987/14987 [==============================] - 13s 898us/step - loss: 0.0145 - acc: 0.9537 - val_loss: 0.0176 - val_acc: 0.9510\n",
      "Epoch 8/10\n",
      "14987/14987 [==============================] - 13s 879us/step - loss: 0.0137 - acc: 0.9560 - val_loss: 0.0182 - val_acc: 0.9508\n",
      "Epoch 9/10\n",
      "14987/14987 [==============================] - 13s 888us/step - loss: 0.0131 - acc: 0.9579 - val_loss: 0.0158 - val_acc: 0.9548\n",
      "Epoch 10/10\n",
      "14987/14987 [==============================] - 13s 875us/step - loss: 0.0125 - acc: 0.9597 - val_loss: 0.0172 - val_acc: 0.9520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14a74bba8>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data = (X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 2s 457us/step\n"
     ]
    }
   ],
   "source": [
    "predicted_ner = model.predict(X_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3684, 150, 11)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3684, 150)\n",
      "(3684, 150, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 2s 491us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.016488607552197804, 0.9429134726524353]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding(Y, X): \n",
    "    unpadded_pred = []\n",
    "    for sent_nbr, sent_ner_predictions in enumerate(Y):\n",
    "        pred = list(map(np.argmax, sent_ner_predictions))\n",
    "        unpadded_pred += [pred[-len(X[sent_nbr]):]]\n",
    "    return unpadded_pred\n",
    "\n",
    "unpad_pred = remove_padding(predicted_ner, X_test_unpadded)\n",
    "ground_truth = remove_padding(Y_test, X_test_unpadded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_cat(Y): \n",
    "    pred = []\n",
    "    for sublist in Y: \n",
    "        pred += [list(map(lambda x : rev_ner_idx[x], sublist))]\n",
    "    return pred \n",
    "\n",
    "pred_cat = num_to_cat(unpad_pred)\n",
    "ground_truth_cat = num_to_cat(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words, _ = build_sequences(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('eval-RNN.txt', 'w+')\n",
    "faults = []\n",
    "for s_idx, sentence in enumerate(test_words): \n",
    "    pred_sentence = ''\n",
    "    predictions = pred_cat[s_idx]\n",
    "    truth = ground_truth_cat[s_idx]\n",
    "    for w_idx, word in enumerate(sentence):\n",
    "        try: \n",
    "            pred_sentence += word + ' ' + predictions[w_idx] + ' ' + truth[w_idx] + '\\n'\n",
    "        except:\n",
    "            pred_sentence += word + ' ' + 'O' + ' ' + 'O' + '\\n' \n",
    "            faults.append([word, s_idx, w_idx])\n",
    "    f.write(pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5720 phrases; found: 5648 phrases; correct: 4064.\n",
      "accuracy:  80.27%; (non-O)\n",
      "accuracy:  94.29%; precision:  71.95%; recall:  71.05%; FB1:  71.50\n",
      "              LOC: precision:  88.37%; recall:  65.72%; FB1:  75.38  1668\n",
      "             MISC: precision:  53.70%; recall:  65.11%; FB1:  58.86  702\n",
      "              ORG: precision:  52.44%; recall:  65.84%; FB1:  58.38  1661\n",
      "              PER: precision:  82.99%; recall:  85.21%; FB1:  84.09  1617\n"
     ]
    }
   ],
   "source": [
    "!python conlleval/conlleval.py < eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 150, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150, 200)          160800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 150, 11)           2211      \n",
      "=================================================================\n",
      "Total params: 40,422,711\n",
      "Trainable params: 163,011\n",
      "Non-trainable params: 40,259,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers \n",
    "from keras.layers import LSTM, Dense, Bidirectional, Dropout \n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "NB_CLASSES = len(ner_set)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(len(total_words) + 2,\n",
    "EMBEDDING_DIM, mask_zero=True, input_length=maxlen))\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "# The default is True\n",
    "model.layers[0].trainable = False\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model.add(Dense(NB_CLASSES + 2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/10\n",
      "14987/14987 [==============================] - 56s 4ms/step - loss: 0.0442 - acc: 0.8702 - val_loss: 0.0283 - val_acc: 0.9204\n",
      "Epoch 2/10\n",
      "14987/14987 [==============================] - 56s 4ms/step - loss: 0.0206 - acc: 0.9346 - val_loss: 0.0217 - val_acc: 0.9352\n",
      "Epoch 3/10\n",
      "14987/14987 [==============================] - 56s 4ms/step - loss: 0.0152 - acc: 0.9521 - val_loss: 0.0169 - val_acc: 0.9499\n",
      "Epoch 4/10\n",
      "14987/14987 [==============================] - 58s 4ms/step - loss: 0.0124 - acc: 0.9607 - val_loss: 0.0145 - val_acc: 0.9584\n",
      "Epoch 5/10\n",
      "14987/14987 [==============================] - 54s 4ms/step - loss: 0.0106 - acc: 0.9661 - val_loss: 0.0128 - val_acc: 0.9638\n",
      "Epoch 6/10\n",
      "14987/14987 [==============================] - 64s 4ms/step - loss: 0.0093 - acc: 0.9702 - val_loss: 0.0117 - val_acc: 0.9664\n",
      "Epoch 7/10\n",
      "14987/14987 [==============================] - 54s 4ms/step - loss: 0.0083 - acc: 0.9734 - val_loss: 0.0110 - val_acc: 0.9688\n",
      "Epoch 8/10\n",
      "14987/14987 [==============================] - 53s 4ms/step - loss: 0.0074 - acc: 0.9761 - val_loss: 0.0139 - val_acc: 0.9566\n",
      "Epoch 9/10\n",
      "14987/14987 [==============================] - 56s 4ms/step - loss: 0.0067 - acc: 0.9786 - val_loss: 0.0128 - val_acc: 0.9654\n",
      "Epoch 10/10\n",
      "14987/14987 [==============================] - 55s 4ms/step - loss: 0.0061 - acc: 0.9803 - val_loss: 0.0095 - val_acc: 0.9728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x151466ef0>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data = (X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 4s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_ner = model.predict(X_test, verbose = 1)\n",
    "\n",
    "unpad_pred = remove_padding(predicted_ner, X_test_unpadded)\n",
    "ground_truth = remove_padding(Y_test, X_test_unpadded)\n",
    "\n",
    "pred_cat = num_to_cat(unpad_pred)\n",
    "ground_truth_cat = num_to_cat(ground_truth)\n",
    "\n",
    "test_words, _ = build_sequences(test_dict)\n",
    "\n",
    "f = open('eval-LSTM.txt', 'w+')\n",
    "faults = []\n",
    "for s_idx, sentence in enumerate(test_words): \n",
    "    pred_sentence = ''\n",
    "    predictions = pred_cat[s_idx]\n",
    "    truth = ground_truth_cat[s_idx]\n",
    "    for w_idx, word in enumerate(sentence):\n",
    "        try: \n",
    "            pred_sentence += word + ' ' + predictions[w_idx] + ' ' + truth[w_idx] + '\\n'\n",
    "        except:\n",
    "            pred_sentence += word + ' ' + 'O' + ' ' + 'O' + '\\n' \n",
    "            faults.append([word, s_idx, w_idx])\n",
    "    f.write(pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46472 tokens with 5748 phrases; found: 5625 phrases; correct: 4683.\n",
      "accuracy:  87.14%; (non-O)\n",
      "accuracy:  96.48%; precision:  83.25%; recall:  81.47%; FB1:  82.35\n",
      "              LOC: precision:  87.97%; recall:  85.21%; FB1:  86.57  1663\n",
      "             MISC: precision:  69.48%; recall:  69.19%; FB1:  69.34  698\n",
      "              ORG: precision:  76.90%; recall:  74.08%; FB1:  75.47  1654\n",
      "              PER: precision:  90.87%; recall:  90.70%; FB1:  90.78  1610\n"
     ]
    }
   ],
   "source": [
    "!python conlleval/conlleval.py < eval-LSTM.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
